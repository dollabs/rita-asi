{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Inverse Reinforcement Learning addresses the problem of '\n",
      "             \"inferring an expert's reward function from demonstrations. \"\n",
      "             'However, in many applications, we not only have access to the '\n",
      "             \"expert's near-optimal behaviour, but we also observe part of her \"\n",
      "             'learning process.\\n'\n",
      "             'In this paper, we propose a new algorithm for this setting, in '\n",
      "             'which the goal is to recover the reward function being optimized '\n",
      "             'by an agent, given a sequence of policies produced during '\n",
      "             'learning. Our approach is based on the assumption that the '\n",
      "             'observed agent is updating her policy parameters along the '\n",
      "             'gradient direction. Then we extend our method to deal with the '\n",
      "             'more realistic scenario where we only have access to a dataset '\n",
      "             'of learning trajectories. For both settings, we provide '\n",
      "             \"theoretical insights into our algorithms' performance. Finally, \"\n",
      "             'we evaluate the approach in a simulated GridWorld environment '\n",
      "             'and on the MuJoCo environments, comparing it with the '\n",
      "             'state-of-the-art baseline.',\n",
      " 'author': 'Giorgia Ramponi, Gianluca Drappo, Marcello Restelli',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/19aa6c6fb4ba9fcf39e893ff1fd5b5bd-Paper.pdf',\n",
      " 'paper': 'Inverse Reinforcement Learning from a Gradient-based Learner',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/19aa6c6fb4ba9fcf39e893ff1fd5b5bd-Supplemental.pdf',\n",
      " 'year': 2020}\n",
      "{'abstract': 'One of the main challenges in imitation learning is determining '\n",
      "             'what action an agent should take when outside the state '\n",
      "             'distribution of the demonstrations. Inverse reinforcement '\n",
      "             'learning (IRL) can enable generalization to new states by '\n",
      "             'learning a parameterized reward function, but these approaches '\n",
      "             'still face uncertainty over the true reward function and '\n",
      "             'corresponding optimal policy.  Existing safe imitation learning '\n",
      "             'approaches based on IRL deal with this uncertainty using a '\n",
      "             'maxmin framework that optimizes a policy under the assumption of '\n",
      "             'an adversarial reward function, whereas risk-neutral IRL '\n",
      "             'approaches either optimize a policy for the mean or MAP reward '\n",
      "             'function.  While completely ignoring risk can lead to overly '\n",
      "             'aggressive and unsafe policies, optimizing in a fully '\n",
      "             'adversarial sense is also problematic as it can lead to overly '\n",
      "             'conservative policies that perform poorly in practice. To '\n",
      "             'provide a bridge between these two extremes,  we propose '\n",
      "             'Bayesian Robust Optimization for Imitation Learning (BROIL). '\n",
      "             'BROIL leverages Bayesian reward function inference and a user '\n",
      "             'specific risk tolerance to efficiently optimize a robust policy '\n",
      "             'that balances expected return and conditional value at risk. Our '\n",
      "             'empirical results show that BROIL provides a natural way to '\n",
      "             'interpolate between return-maximizing and risk-minimizing '\n",
      "             'behaviors and outperforms existing risk-sensitive and '\n",
      "             'risk-neutral inverse reinforcement learning algorithms.',\n",
      " 'author': 'Daniel Brown, Scott Niekum, Marek Petrik',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/1a669e81c8093745261889539694be7f-Paper.pdf',\n",
      " 'paper': 'Bayesian Robust Optimization for Imitation Learning',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/1a669e81c8093745261889539694be7f-Supplemental.pdf',\n",
      " 'year': 2020}\n",
      "{'abstract': 'The problem of inverse reinforcement learning (IRL) is relevant '\n",
      "             'to a variety of tasks including value alignment and robot '\n",
      "             'learning from demonstration. Despite significant algorithmic '\n",
      "             'contributions in recent years, IRL remains an ill-posed problem '\n",
      "             'at its core; multiple reward functions coincide with the '\n",
      "             'observed behavior and the actual reward function is not '\n",
      "             'identifiable without prior knowledge or supplementary '\n",
      "             'information. This paper presents an IRL framework called '\n",
      "             'Bayesian optimization-IRL (BO-IRL) which identifies multiple '\n",
      "             'solutions that are consistent with the expert demonstrations by '\n",
      "             'efficiently exploring the reward function space. BO-IRL achieves '\n",
      "             'this by utilizing Bayesian Optimization along with our newly '\n",
      "             'proposed kernel that (a) projects the parameters of policy '\n",
      "             'invariant reward functions to a single point in a latent space '\n",
      "             'and (b) ensures nearby points in the latent space correspond to '\n",
      "             'reward functions yielding similar likelihoods. This projection '\n",
      "             'allows the use of standard stationary kernels in the latent '\n",
      "             'space to capture the correlations present across the reward '\n",
      "             'function space. Empirical results on synthetic and real-world '\n",
      "             'environments (model-free and model-based) show that BO-IRL '\n",
      "             'discovers multiple reward functions while minimizing the number '\n",
      "             'of expensive exact policy optimizations.',\n",
      " 'author': 'Sreejith Balakrishnan, Quoc Phong Nguyen, Bryan Kian Hsiang Low, '\n",
      "           'Harold Soh',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/2bba9f4124283edd644799e0cecd45ca-Paper.pdf',\n",
      " 'paper': 'Efficient Exploration of Reward Functions in Inverse Reinforcement '\n",
      "          'Learning via Bayesian Optimization',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/2bba9f4124283edd644799e0cecd45ca-Supplemental.zip',\n",
      " 'year': 2020}\n",
      "{'abstract': 'It is often difficult to hand-specify what the correct reward '\n",
      "             'function is for a task, so researchers have instead aimed to '\n",
      "             'learn reward functions from human behavior or feedback. The '\n",
      "             'types of behavior interpreted as evidence of the reward function '\n",
      "             \"have expanded greatly in recent years. We've gone from \"\n",
      "             'demonstrations, to comparisons, to reading into the information '\n",
      "             'leaked when the human is pushing the robot away or turning it '\n",
      "             'off. And surely, there is more to come. How will a robot make '\n",
      "             'sense of all these diverse types of behavior? Our key '\n",
      "             'observation is that different types of behavior can be '\n",
      "             'interpreted in a single unifying formalism - as a '\n",
      "             'reward-rational choice that the human is making, often '\n",
      "             'implicitly. We use this formalism to survey prior work through a '\n",
      "             'unifying lens, and discuss its potential use as a recipe for '\n",
      "             'interpreting new sources of information that are yet to be '\n",
      "             'uncovered. ',\n",
      " 'author': 'Hong Jun Jeon, Smitha Milli, Anca Dragan',\n",
      " 'keywords': ['learn reward function'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/2f10c1578a0706e06b6d7db6f0b4a6af-Paper.pdf',\n",
      " 'paper': 'Reward-rational (implicit) choice: A unifying formalism for reward '\n",
      "          'learning',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/2f10c1578a0706e06b6d7db6f0b4a6af-Supplemental.pdf',\n",
      " 'year': 2020}\n",
      "{'abstract': 'One difficulty in using artificial agents for human-assistive '\n",
      "             'applications lies in the challenge of accurately assisting with '\n",
      "             \"a person's goal(s).  Existing methods tend to rely on inferring \"\n",
      "             \"the human's goal, which is challenging when there are many \"\n",
      "             'potential goals or when the set of candidate goals is difficult '\n",
      "             'to identify. We propose a new paradigm for assistance by instead '\n",
      "             \"increasing the human's ability to control their environment, and \"\n",
      "             'formalize this approach by augmenting reinforcement learning '\n",
      "             'with human empowerment. This task-agnostic objective increases '\n",
      "             \"the person's autonomy and ability to achieve any eventual state. \"\n",
      "             'We test our approach against assistance based on goal inference, '\n",
      "             'highlighting scenarios where our method overcomes failure modes '\n",
      "             'stemming from goal ambiguity or misspecification. As existing '\n",
      "             'methods for estimating empowerment in continuous domains are '\n",
      "             'computationally hard, precluding its use in real time learned '\n",
      "             'assistance, we also propose an efficient empowerment-inspired '\n",
      "             'proxy metric. Using this, we are able to successfully '\n",
      "             'demonstrate our method in a shared autonomy user study for a '\n",
      "             'challenging simulated teleoperation task with human-in-the-loop '\n",
      "             'training. ',\n",
      " 'author': 'Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter '\n",
      "           'Abbeel, Anca Dragan',\n",
      " 'keywords': ['goal inference'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/30de9ece7cf3790c8c39ccff1a044209-Paper.pdf',\n",
      " 'paper': 'AvE: Assistance via Empowerment',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/30de9ece7cf3790c8c39ccff1a044209-Supplemental.zip',\n",
      " 'year': 2020}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Real-world networks, especially the ones that emerge due to '\n",
      "             'actions of animate agents (e.g. humans, animals), are the result '\n",
      "             'of underlying strategic mechanisms aimed at maximizing '\n",
      "             'individual or collective benefits. Learning approaches built to '\n",
      "             'capture these strategic insights would gain interpretability and '\n",
      "             'flexibility benefits that are required to generalize beyond '\n",
      "             'observations.\\n'\n",
      "             'To this end, we consider a game-theoretic formalism of network '\n",
      "             'emergence that accounts for the underlying strategic mechanisms '\n",
      "             'and take it to the observed data. \\n'\n",
      "             'We propose MINE (Multi-agent Inverse models of Network Emergence '\n",
      "             'mechanism), a new learning framework that solves Markov-Perfect '\n",
      "             'network emergence games using multi-agent inverse reinforcement '\n",
      "             \"learning. MINE jointly discovers agents' strategy profiles in \"\n",
      "             'the form of network emergence policy and the latent payoff '\n",
      "             'mechanism in the form of learned reward function. In the '\n",
      "             'experiments, we demonstrate that MINE learns versatile payoff '\n",
      "             'mechanisms that: highly correlates with the ground truth for a '\n",
      "             'synthetic case; can be used to analyze the observed network '\n",
      "             'structure; and enable effective transfer in specific settings. '\n",
      "             'Further, we show that the network emergence game as a learned '\n",
      "             'model supports meaningful strategic predictions, thereby '\n",
      "             'signifying its applicability to a variety of  network analysis '\n",
      "             'tasks.',\n",
      " 'author': 'Rakshit Trivedi, Hongyuan Zha',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/4bb236de7787ceedafdff83bb8ea4710-Paper.pdf',\n",
      " 'paper': 'Learning Strategic Network Emergence Games',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/4bb236de7787ceedafdff83bb8ea4710-Supplemental.pdf',\n",
      " 'year': 2020}\n",
      "{'abstract': 'One of the key reasons for the high sample complexity in '\n",
      "             'reinforcement learning (RL) is the inability to transfer '\n",
      "             'knowledge from one task to another. In standard multi-task RL '\n",
      "             'settings, low-reward data collected while trying to solve one '\n",
      "             'task provides little to no signal for solving that particular '\n",
      "             'task and is hence effectively wasted. However, we argue that '\n",
      "             'this data, which is uninformative for one task, is likely a rich '\n",
      "             'source of information for other tasks. To leverage this insight '\n",
      "             'and efficiently reuse data, we present Generalized Hindsight: an '\n",
      "             'approximate inverse reinforcement learning technique for '\n",
      "             'relabeling behaviors with the right tasks. Intuitively, given a '\n",
      "             'behavior generated under one task, Generalized Hindsight returns '\n",
      "             'a different task that the behavior is better suited for. Then, '\n",
      "             'the behavior is relabeled with this new task before being used '\n",
      "             'by an off-policy RL optimizer. Compared to standard relabeling '\n",
      "             'techniques, Generalized Hindsight provides a substantially more '\n",
      "             'efficient re-use of samples, which we empirically demonstrate on '\n",
      "             'a suite of multi-task navigation and manipulation tasks.',\n",
      " 'author': 'Alexander Li, Lerrel Pinto, Pieter Abbeel',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/57e5cb96e22546001f1d6520ff11d9ba-Paper.pdf',\n",
      " 'paper': 'Generalized Hindsight for Reinforcement Learning',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/57e5cb96e22546001f1d6520ff11d9ba-Supplemental.zip',\n",
      " 'year': 2020}\n",
      "{'abstract': 'This paper develops a Pontryagin differentiable programming '\n",
      "             '(PDP) methodology, which establishes a unified framework to '\n",
      "             'solve a broad class of learning and control tasks. The PDP  '\n",
      "             'distinguishes from existing methods by two novel techniques: '\n",
      "             \"first, we  differentiate through Pontryagin's Maximum \"\n",
      "             'Principle,  and this allows  to obtain the analytical derivative '\n",
      "             'of a  trajectory with respect to tunable parameters within an '\n",
      "             'optimal control system,  enabling end-to-end learning of   '\n",
      "             'dynamics, policies, or/and control objective functions; and '\n",
      "             'second, we propose an auxiliary control system in the backward '\n",
      "             'pass of the PDP framework, and  the output of this auxiliary '\n",
      "             'control system is the analytical derivative of the original '\n",
      "             \"system's trajectory with respect to the  parameters, which can \"\n",
      "             'be iteratively solved using standard control tools. We '\n",
      "             'investigate three learning modes of the PDP: inverse '\n",
      "             'reinforcement learning,  system identification, and  '\n",
      "             'control/planning. We demonstrate the capability of the PDP in '\n",
      "             'each learning mode on different high-dimensional systems, '\n",
      "             'including multilink robot arm,  6-DoF maneuvering UAV, and 6-DoF '\n",
      "             'rocket powered landing.',\n",
      " 'author': 'Wanxin Jin, Zhaoran Wang, Zhuoran Yang, Shaoshuai Mou',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf',\n",
      " 'paper': 'Pontryagin Differentiable Programming: An End-to-End Learning and '\n",
      "          'Control Framework',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/5a7b238ba0f6502e5d6be14424b20ded-Supplemental.pdf',\n",
      " 'year': 2020}\n",
      "{'abstract': 'One of the common ways children learn is by mimicking adults. '\n",
      "             'Imitation learning focuses on learning policies with suitable '\n",
      "             'performance from demonstrations generated by an expert, with an '\n",
      "             'unspecified performance measure, and unobserved reward signal. '\n",
      "             'Popular methods for imitation learning start by either directly '\n",
      "             'mimicking the behavior policy of an expert (behavior cloning) or '\n",
      "             'by learning a reward function that prioritizes observed expert '\n",
      "             'trajectories (inverse reinforcement learning). However, these '\n",
      "             'methods rely on the assumption that covariates used by the '\n",
      "             'expert to determine her/his actions are fully observed. In this '\n",
      "             'paper, we relax this assumption and study imitation learning '\n",
      "             'when sensory inputs of the learner and the expert differ. First, '\n",
      "             'we provide a non-parametric, graphical criterion that is '\n",
      "             'complete (both necessary and sufficient) for determining the '\n",
      "             'feasibility of imitation from the combinations of demonstration '\n",
      "             'data and qualitative assumptions about the underlying '\n",
      "             'environment, represented in the form of a causal model. We then '\n",
      "             'show that when such a criterion does not hold, imitation could '\n",
      "             'still be feasible by exploiting quantitative knowledge of the '\n",
      "             'expert trajectories. Finally, we develop an efficient procedure '\n",
      "             \"for learning the imitating policy from experts' trajectories. \",\n",
      " 'author': 'Junzhe Zhang, Daniel Kumor, Elias Bareinboim',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/8fdd149fcaa7058caccc9c4ad5b0d89a-Paper.pdf',\n",
      " 'paper': 'Causal Imitation Learning With Unobserved Confounders',\n",
      " 'supplemental': 'None',\n",
      " 'year': 2020}\n",
      "{'abstract': 'Popular Maximum Entropy Inverse Reinforcement Learning '\n",
      "             'approaches require the computation of expected state visitation '\n",
      "             'frequencies for the optimal policy under an estimate of the '\n",
      "             'reward function. This usually requires intermediate value '\n",
      "             'estimation in the inner loop of the algorithm, slowing down '\n",
      "             'convergence considerably. In this work, we introduce a novel '\n",
      "             'class of algorithms that only needs to solve the MDP underlying '\n",
      "             'the demonstrated behavior once to recover the expert policy. '\n",
      "             'This is possible through a formulation that exploits a '\n",
      "             'probabilistic behavior assumption for the demonstrations within '\n",
      "             'the structure of Q-learning. We propose Inverse Action-value '\n",
      "             'Iteration which is able to fully recover an underlying reward of '\n",
      "             'an external agent in closed-form analytically. We further '\n",
      "             'provide an accompanying class of sampling-based variants which '\n",
      "             'do not depend on a model of the environment. We show how to '\n",
      "             'extend this class of algorithms to continuous state-spaces via '\n",
      "             'function approximation and how to estimate a corresponding '\n",
      "             'action-value function, leading to a policy as close as possible '\n",
      "             'to the policy of the external agent, while optionally satisfying '\n",
      "             'a list of predefined hard constraints. We evaluate the resulting '\n",
      "             'algorithms called Inverse Action-value Iteration, Inverse '\n",
      "             'Q-learning and Deep Inverse Q-learning on the Objectworld '\n",
      "             'benchmark, showing a speedup of up to several orders of '\n",
      "             'magnitude compared to (Deep) Max-Entropy algorithms. We further '\n",
      "             'apply Deep Constrained Inverse Q-learning on the task of '\n",
      "             'learning autonomous lane-changes in the open-source simulator '\n",
      "             'SUMO achieving competent driving after training on data '\n",
      "             'corresponding to 30 minutes of demonstrations.',\n",
      " 'author': 'Gabriel Kalweit, Maria Huegle, Moritz Werling, Joschka Boedecker',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/a4c42bfd5f5130ddf96e34a036c75e0a-Paper.pdf',\n",
      " 'paper': 'Deep Inverse Q-learning with Constraints',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/a4c42bfd5f5130ddf96e34a036c75e0a-Supplemental.pdf',\n",
      " 'year': 2020}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'People routinely infer the goals of others by observing their '\n",
      "             'actions over time. Remarkably, we can do so even when those '\n",
      "             'actions lead to failure, enabling us to assist others when we '\n",
      "             'detect that they might not achieve their goals. How might we '\n",
      "             'endow machines with similar capabilities? Here we present an '\n",
      "             'architecture capable of inferring an agent’s goals online from '\n",
      "             'both optimal and non-optimal sequences of actions. Our '\n",
      "             'architecture models agents as boundedly-rational planners that '\n",
      "             'interleave search with execution by replanning, thereby '\n",
      "             'accounting for sub-optimal behavior. These models are specified '\n",
      "             'as probabilistic programs, allowing us to represent and perform '\n",
      "             \"efficient Bayesian inference over an agent's goals and internal \"\n",
      "             'planning processes. To perform such inference, we develop '\n",
      "             'Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo '\n",
      "             'algorithm that exploits the online replanning assumption of '\n",
      "             'these models, limiting computation by incrementally extending '\n",
      "             'inferred plans as new actions are observed. We present '\n",
      "             'experiments showing that this modeling and inference '\n",
      "             'architecture outperforms Bayesian inverse reinforcement learning '\n",
      "             'baselines, accurately inferring goals from both optimal and '\n",
      "             'non-optimal trajectories involving failure and back-tracking, '\n",
      "             'while generalizing across domains with compositional structure '\n",
      "             'and sparse rewards.',\n",
      " 'author': 'Tan Zhi-Xuan, Jordyn Mann, Tom Silver, Josh Tenenbaum, Vikash '\n",
      "           'Mansinghka',\n",
      " 'keywords': ['inverse reinforcement learning', 'infer the goal'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2020/file/df3aebc649f9e3b674eeb790a4da224e-Paper.pdf',\n",
      " 'paper': 'Online Bayesian Goal Inference for Boundedly Rational Planning '\n",
      "          'Agents',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2020/file/df3aebc649f9e3b674eeb790a4da224e-Supplemental.zip',\n",
      " 'year': 2020}\n",
      "11\n",
      "{'abstract': 'In collective decision making, members of a group need to '\n",
      "             'coordinate their actions in order to achieve a desirable '\n",
      "             'outcome. When there is no direct communication between group '\n",
      "             \"members, one should decide based on inferring others' intentions \"\n",
      "             \"from their actions. The inference of others' intentions is \"\n",
      "             'called \"theory of mind\" and can involve different levels of '\n",
      "             'reasoning, from a single inference on a hidden variable to '\n",
      "             'considering others partially or fully optimal and reasoning '\n",
      "             \"about their actions conditioned on one's own actions (levels of \"\n",
      "             '“theory of mind”). In this paper, we present a new Bayesian '\n",
      "             'theory of collective decision making based on a simple yet most '\n",
      "             'commonly observed behavior: conformity. We show that such a '\n",
      "             'Bayesian framework allows one to achieve any level of theory of '\n",
      "             'mind in collective decision making. The viability of our '\n",
      "             'framework is demonstrated on two different experiments, a '\n",
      "             \"consensus task with 120 subjects and a volunteer's dilemma task \"\n",
      "             'with 29 subjects, each with multiple conditions.',\n",
      " 'author': 'Koosha Khalvati, Saghar Mirbagheri, Seongmin A. Park, Jean-Claude '\n",
      "           'Dreher, Rajesh PN Rao',\n",
      " 'keywords': ['theory of mind'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2019/file/15212f24321aa2c3dc8e9acf820f3c15-Paper.pdf',\n",
      " 'paper': 'A Bayesian Theory of Conformity in Collective Decision Making',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2019/file/15212f24321aa2c3dc8e9acf820f3c15-Supplemental.zip',\n",
      " 'year': 2019}\n",
      "{'abstract': 'Imitation Learning (IL) has been successfully applied to complex '\n",
      "             'sequential decision-making problems where standard Reinforcement '\n",
      "             'Learning (RL) algorithms fail. A number of recent methods extend '\n",
      "             'IL to few-shot learning scenarios, where a meta-trained policy '\n",
      "             'learns to quickly master new tasks using limited demonstrations. '\n",
      "             'However, although Inverse Reinforcement Learning (IRL) often '\n",
      "             'outperforms Behavioral Cloning (BC) in terms of imitation '\n",
      "             'quality, most of these approaches build on BC due to its simple '\n",
      "             'optimization objective. In this work, we propose SMILe, a '\n",
      "             'scalable framework for Meta Inverse Reinforcement Learning '\n",
      "             '(Meta-IRL) based on maximum entropy IRL, which can learn '\n",
      "             'high-quality policies from few demonstrations. We examine the '\n",
      "             'efficacy of our method on a variety of high-dimensional '\n",
      "             'simulated continuous control tasks and observe that SMILe '\n",
      "             'significantly outperforms Meta-BC. Furthermore, we observe that '\n",
      "             'SMILe performs comparably or outperforms Meta-DAgger, while '\n",
      "             'being applicable in the state-only setting and not requiring '\n",
      "             'online experts. To our knowledge, our approach is the first '\n",
      "             'efficient method for Meta-IRL that scales to the function '\n",
      "             'approximator setting. For datasets and reproducing results '\n",
      "             'please refer to '\n",
      "             'https://github.com/KamyarGh/rlswiss/blob/master/reproducing/smilepaper.md '\n",
      "             '.',\n",
      " 'author': 'Seyed Kamyar Seyed Ghasemipour, Shixiang (Shane) Gu, Richard Zemel',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2019/file/2b8f621e9244cea5007bac8f5d50e476-Paper.pdf',\n",
      " 'paper': 'SMILe: Scalable Meta Inverse Reinforcement Learning through '\n",
      "          'Context-Conditional Policies',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2019/file/2b8f621e9244cea5007bac8f5d50e476-Supplemental.zip',\n",
      " 'year': 2019}\n",
      "{'abstract': 'Reinforcement learning demands a reward function, which is often '\n",
      "             'difficult to provide or design in real world applications. While '\n",
      "             'inverse reinforcement learning (IRL) holds promise for '\n",
      "             'automatically learning reward functions from demonstrations, '\n",
      "             'several major challenges remain. First, existing IRL methods '\n",
      "             'learn reward functions from scratch, requiring large numbers of '\n",
      "             'demonstrations to correctly infer the reward for each task the '\n",
      "             'agent may need to perform. Second, and more subtly, existing '\n",
      "             'methods typically assume demonstrations for one, isolated '\n",
      "             'behavior or task, while in practice, it is significantly more '\n",
      "             'natural and scalable to provide datasets of heterogeneous '\n",
      "             'behaviors. To this end, we propose a deep latent variable model '\n",
      "             'that is capable of learning rewards from unstructured, '\n",
      "             'multi-task demonstration data, and critically, use this '\n",
      "             'experience to infer robust rewards for new, structurally-similar '\n",
      "             'tasks from a single demonstration. Our experiments on multiple '\n",
      "             'continuous control tasks demonstrate the effectiveness of our '\n",
      "             'approach compared to state-of-the-art imitation and inverse '\n",
      "             'reinforcement learning methods.',\n",
      " 'author': 'Lantao Yu, Tianhe Yu, Chelsea Finn, Stefano Ermon',\n",
      " 'keywords': ['inverse reinforcement learning', 'learn reward function'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2019/file/30de24287a6d8f07b37c716ad51623a7-Paper.pdf',\n",
      " 'paper': 'Meta-Inverse Reinforcement Learning with Probabilistic Context '\n",
      "          'Variables',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2019/file/30de24287a6d8f07b37c716ad51623a7-Supplemental.zip',\n",
      " 'year': 2019}\n",
      "{'abstract': 'Inverse reinforcement learning (IRL) enables an agent to learn '\n",
      "             'complex behavior by observing demonstrations from a '\n",
      "             '(near-)optimal policy. The typical assumption is that the '\n",
      "             \"learner's goal is to match the teacher’s demonstrated behavior. \"\n",
      "             'In this paper, we consider the setting where the learner has its '\n",
      "             'own preferences that it additionally takes into consideration. '\n",
      "             'These preferences can for example capture behavioral biases, '\n",
      "             'mismatched worldviews, or physical constraints. We study two '\n",
      "             'teaching approaches: learner-agnostic teaching, where the '\n",
      "             'teacher provides demonstrations from an optimal policy ignoring '\n",
      "             \"the learner's preferences, and learner-aware teaching, where the \"\n",
      "             'teacher accounts for the learner’s preferences. We design '\n",
      "             'learner-aware teaching algorithms and show that significant '\n",
      "             'performance improvements can be achieved over learner-agnostic '\n",
      "             'teaching.',\n",
      " 'author': 'Sebastian Tschiatschek, Ahana Ghosh, Luis Haug, Rati Devidze, '\n",
      "           'Adish Singla',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2019/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf',\n",
      " 'paper': 'Learner-aware Teaching: Inverse Reinforcement Learning with '\n",
      "          'Preferences and Constraints',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2019/file/3de568f8597b94bda53149c7d7f5958c-Supplemental.zip',\n",
      " 'year': 2019}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Making decisions in the presence of a strategic opponent '\n",
      "             'requires one to take into account the opponent’s ability to '\n",
      "             'actively mask its intended objective. To describe such strategic '\n",
      "             'situations, we introduce the non-cooperative inverse '\n",
      "             'reinforcement learning (N-CIRL) formalism. The N-CIRL formalism '\n",
      "             'consists of two agents with completely misaligned objectives, '\n",
      "             'where only one of the agents knows the true objective function. '\n",
      "             'Formally, we model the N-CIRL formalism as a zero-sum Markov '\n",
      "             'game with one-sided incomplete information. Through interacting '\n",
      "             'with the more informed player, the less informed player attempts '\n",
      "             'to both infer and optimize the true objective function. As a '\n",
      "             'result of the one-sided incomplete information, the multi-stage '\n",
      "             'game can be decomposed into a sequence of single- stage games '\n",
      "             'expressed by a recursive formula. Solving this recursive formula '\n",
      "             'yields the value of the N-CIRL game and the more informed '\n",
      "             'player’s equilibrium strategy. Another recursive formula, '\n",
      "             'constructed by forming an auxiliary game, termed the dual game, '\n",
      "             'yields the less informed player’s strategy. Building upon these '\n",
      "             'two recursive formulas, we develop a computationally tractable '\n",
      "             'algorithm to approximately solve for the equilibrium strategies. '\n",
      "             'Finally, we demonstrate the benefits of our N-CIRL formalism '\n",
      "             'over the existing multi-agent IRL formalism via extensive '\n",
      "             'numerical simulation in a novel cyber security setting.',\n",
      " 'author': 'Xiangyuan Zhang, Kaiqing Zhang, Erik Miehling, Tamer Basar',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2019/file/56bd37d3a2fda0f2f41925019c81011d-Paper.pdf',\n",
      " 'paper': 'Non-Cooperative Inverse Reinforcement Learning',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2019/file/56bd37d3a2fda0f2f41925019c81011d-Supplemental.zip',\n",
      " 'year': 2019}\n",
      "{'abstract': 'We consider differentially private algorithms for reinforcement '\n",
      "             'learning in continuous spaces, such that neighboring reward '\n",
      "             'functions are indistinguishable. This protects the reward '\n",
      "             'information from being exploited by methods such as inverse '\n",
      "             'reinforcement learning. Existing studies that guarantee '\n",
      "             'differential privacy are not extendable to infinite state '\n",
      "             'spaces, as the noise level to ensure privacy will scale '\n",
      "             'accordingly to infinity. Our aim is to protect the value '\n",
      "             'function approximator, without regard to the number of states '\n",
      "             'queried to the function. It is achieved by adding functional '\n",
      "             'noise to the value function iteratively in the training. We show '\n",
      "             'rigorous privacy guarantees by a series of analyses on the '\n",
      "             'kernel of the noise space, the probabilistic bound of such noise '\n",
      "             'samples, and the composition over the iterations. We gain '\n",
      "             \"insight into the utility analysis by proving the algorithm's \"\n",
      "             'approximate optimality when the state space is discrete. '\n",
      "             'Experiments corroborate our theoretical findings and show '\n",
      "             'improvement over existing approaches.',\n",
      " 'author': 'Baoxiang Wang, Nidhi Hegde',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2019/file/6646b06b90bd13dabc11ddba01270d23-Paper.pdf',\n",
      " 'paper': 'Privacy-Preserving Q-Learning with Functional Noise in Continuous '\n",
      "          'Spaces',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2019/file/6646b06b90bd13dabc11ddba01270d23-Supplemental.zip',\n",
      " 'year': 2019}\n",
      "{'abstract': 'We consider the problem of using logged data to make predictions '\n",
      "             \"about what would happen if we changed the `rules of the game' in \"\n",
      "             'a multi-agent system. This task is difficult because in many '\n",
      "             'cases we observe actions individuals take but not their private '\n",
      "             'information or their full reward functions. In addition, agents '\n",
      "             'are strategic, so when the rules change, they will also change '\n",
      "             'their actions. Existing methods (e.g. structural estimation, '\n",
      "             \"inverse reinforcement learning) assume that agents' behavior \"\n",
      "             'comes from optimizing some utility or that the system is in '\n",
      "             'equilibrium. They make counterfactual predictions by using '\n",
      "             'observed actions to learn the underlying utility function '\n",
      "             '(a.k.a. type) and then solving for the equilibrium of the '\n",
      "             'counterfactual environment. This approach imposes heavy '\n",
      "             'assumptions such as the rationality of the agents being observed '\n",
      "             \"and a correct model of the environment and agents' utility \"\n",
      "             'functions. We propose a method for analyzing the sensitivity of '\n",
      "             'counterfactual conclusions to violations of these assumptions, '\n",
      "             'which we call robust multi-agent counterfactual prediction '\n",
      "             '(RMAC). We provide a first-order method for computing RMAC '\n",
      "             'bounds. We apply RMAC to classic environments in market design: '\n",
      "             'auctions, school choice, and social choice.',\n",
      " 'author': 'Alexander Peysakhovich, Christian Kroer, Adam Lerer',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2019/file/fc9b003bb003a298c2ad0d05e4342bdc-Paper.pdf',\n",
      " 'paper': 'Robust Multi-agent Counterfactual Prediction',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2019/file/fc9b003bb003a298c2ad0d05e4342bdc-Supplemental.zip',\n",
      " 'year': 2019}\n",
      "18\n",
      "{'abstract': 'Goal-oriented dialog has been given attention due to its '\n",
      "             'numerous applications in artificial intelligence.\\n'\n",
      "             'Goal-oriented dialogue tasks occur when a questioner asks an '\n",
      "             'action-oriented question and an answerer responds with the '\n",
      "             'intent of letting the questioner know a correct action to '\n",
      "             'take. \\n'\n",
      "             'To ask the adequate question, deep learning and reinforcement '\n",
      "             'learning have been recently applied. \\n'\n",
      "             'However, these approaches struggle to find a competent recurrent '\n",
      "             'neural questioner, owing to the complexity of learning a series '\n",
      "             'of sentences.\\n'\n",
      "             'Motivated by theory of mind, we propose \"Answerer in '\n",
      "             'Questioner\\'s Mind\" (AQM), a novel information theoretic '\n",
      "             'algorithm for goal-oriented dialog. \\n'\n",
      "             'With AQM, a questioner asks and infers based on an approximated '\n",
      "             'probabilistic model of the answerer.\\n'\n",
      "             'The questioner figures out the answerer’s intention via '\n",
      "             'selecting a plausible question by explicitly calculating the '\n",
      "             'information gain of the candidate intentions and possible '\n",
      "             'answers to each question.\\n'\n",
      "             'We test our framework on two goal-oriented visual dialog tasks: '\n",
      "             '\"MNIST Counting Dialog\" and \"GuessWhat?!\".\\n'\n",
      "             'In our experiments, AQM outperforms comparative algorithms by a '\n",
      "             'large margin.',\n",
      " 'author': 'Sang-Woo Lee, Yu-Jung Heo, Byoung-Tak Zhang',\n",
      " 'keywords': ['theory of mind'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2018/file/0829424ffa0d3a2547b6c9622c77de03-Paper.pdf',\n",
      " 'paper': \"Answerer in Questioner's Mind: Information Theoretic Approach to \"\n",
      "          'Goal-Oriented Visual Dialog',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2018/file/0829424ffa0d3a2547b6c9622c77de03-Supplemental.zip',\n",
      " 'year': 2018}\n",
      "{'abstract': 'Imitation learning algorithms can be used to learn a policy from '\n",
      "             'expert demonstrations without access to a reward signal. '\n",
      "             'However, most existing approaches are not applicable in '\n",
      "             'multi-agent settings due to the existence of multiple (Nash) '\n",
      "             'equilibria and non-stationary environments.\\n'\n",
      "             'We propose a new framework for multi-agent imitation learning '\n",
      "             'for general Markov games, where we build upon a generalized '\n",
      "             'notion of inverse reinforcement learning. We further introduce a '\n",
      "             'practical multi-agent actor-critic algorithm with good empirical '\n",
      "             'performance. Our method can be used to imitate complex behaviors '\n",
      "             'in high-dimensional environments with multiple cooperative or '\n",
      "             'competing agents.',\n",
      " 'author': 'Jiaming Song, Hongyu Ren, Dorsa Sadigh, Stefano Ermon',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2018/file/240c945bb72980130446fc2b40fbb8e0-Paper.pdf',\n",
      " 'paper': 'Multi-Agent Generative Adversarial Imitation Learning',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2018/file/240c945bb72980130446fc2b40fbb8e0-Supplemental.zip',\n",
      " 'year': 2018}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Methods for learning from demonstration (LfD) have shown success '\n",
      "             'in acquiring behavior policies by imitating a user. However, '\n",
      "             'even for a single task, LfD may require numerous demonstrations. '\n",
      "             'For versatile agents that must learn many tasks via '\n",
      "             'demonstration, this process would substantially burden the user '\n",
      "             'if each task were learned in isolation. To address this '\n",
      "             'challenge, we introduce the novel problem of lifelong learning '\n",
      "             'from demonstration, which allows the agent to continually build '\n",
      "             'upon knowledge learned from previously demonstrated tasks to '\n",
      "             'accelerate the learning of new tasks, reducing the amount of '\n",
      "             'demonstrations required. As one solution to this problem, we '\n",
      "             'propose the first lifelong learning approach to inverse '\n",
      "             'reinforcement learning, which learns consecutive tasks via '\n",
      "             'demonstration, continually transferring knowledge between tasks '\n",
      "             'to improve performance.',\n",
      " 'author': 'Jorge Mendez, Shashank Shivkumar, Eric Eaton',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2018/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Paper.pdf',\n",
      " 'paper': 'Lifelong Inverse Reinforcement Learning',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2018/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Supplemental.zip',\n",
      " 'year': 2018}\n",
      "{'abstract': \"Learning near-optimal behaviour from an expert's demonstrations \"\n",
      "             'typically relies on the assumption that the learner knows the '\n",
      "             'features that the true reward function depends on. In this '\n",
      "             'paper, we study the problem of learning from demonstrations in '\n",
      "             'the setting where this is not the case, i.e., where there is a '\n",
      "             'mismatch between the worldviews of the learner and the expert. '\n",
      "             'We introduce a natural quantity, the teaching risk, which '\n",
      "             'measures the potential suboptimality of policies that look '\n",
      "             'optimal to the learner in this setting. We show that bounds on '\n",
      "             'the teaching risk guarantee that the learner is able to find a '\n",
      "             'near-optimal policy using standard algorithms based on inverse '\n",
      "             'reinforcement learning. Based on these findings, we suggest a '\n",
      "             'teaching scheme in which the expert can decrease the teaching '\n",
      "             \"risk by updating the learner's worldview, and thus ultimately \"\n",
      "             'enable her to find a near-optimal policy.',\n",
      " 'author': 'Luis Haug, Sebastian Tschiatschek, Adish Singla',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2018/file/4928e7510f45da6575b04a28519c09ed-Paper.pdf',\n",
      " 'paper': 'Teaching Inverse Reinforcement Learners via Features and '\n",
      "          'Demonstrations',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2018/file/4928e7510f45da6575b04a28519c09ed-Supplemental.zip',\n",
      " 'year': 2018}\n",
      "{'abstract': 'Social goods, such as healthcare, smart city, and information '\n",
      "             'networks, often produce ordered event data in continuous time. '\n",
      "             'The generative processes of these event data can be very '\n",
      "             'complex, requiring flexible models to capture their dynamics. '\n",
      "             'Temporal point processes offer an elegant framework for modeling '\n",
      "             'event data without discretizing the time. However, the existing '\n",
      "             'maximum-likelihood-estimation (MLE) learning paradigm requires '\n",
      "             'hand-crafting the intensity function beforehand and cannot '\n",
      "             'directly monitor the goodness-of-fit of the estimated model in '\n",
      "             'the process of training. To alleviate the risk of '\n",
      "             'model-misspecification in MLE, we propose to generate samples '\n",
      "             'from the generative model and monitor the quality of the samples '\n",
      "             'in the process of training until the samples and the real data '\n",
      "             'are indistinguishable. We take inspiration from reinforcement '\n",
      "             'learning (RL) and treat the generation of each event as the '\n",
      "             'action taken by a stochastic policy. We parameterize the policy '\n",
      "             'as a flexible recurrent neural network and gradually improve the '\n",
      "             'policy to mimic the observed event distribution. Since the '\n",
      "             'reward function is unknown in this setting, we uncover an '\n",
      "             'analytic and nonparametric form of the reward function using an '\n",
      "             'inverse reinforcement learning formulation. This new RL '\n",
      "             'framework allows us to derive an efficient policy gradient '\n",
      "             'algorithm for learning flexible point process models, and we '\n",
      "             'show that it performs well in both synthetic and real data.',\n",
      " 'author': 'Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, Le Song',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2018/file/5d50d22735a7469266aab23fd8aeb536-Paper.pdf',\n",
      " 'paper': 'Learning Temporal Point Processes via Reinforcement Learning',\n",
      " 'supplemental': 'None',\n",
      " 'year': 2018}\n",
      "{'abstract': 'Inferring intent from observed behavior has been studied '\n",
      "             'extensively within the frameworks of Bayesian inverse planning '\n",
      "             'and inverse reinforcement learning. These methods infer a goal '\n",
      "             'or reward function that best explains the actions of the '\n",
      "             'observed agent, typically a human demonstrator. Another agent '\n",
      "             'can use this inferred intent to predict, imitate, or assist the '\n",
      "             'human user. However, a central assumption in inverse '\n",
      "             'reinforcement learning is that the demonstrator is close to '\n",
      "             'optimal. While models of suboptimal behavior exist, they '\n",
      "             'typically assume that suboptimal actions are the result of some '\n",
      "             'type of random noise or a known cognitive bias, like temporal '\n",
      "             'inconsistency. In this paper, we take an alternative approach, '\n",
      "             'and model suboptimal behavior as the result of internal model '\n",
      "             'misspecification: the reason that user actions might deviate '\n",
      "             'from near-optimal actions is that the user has an incorrect set '\n",
      "             'of beliefs about the rules -- the dynamics -- governing how '\n",
      "             'actions affect the environment. Our insight is that while '\n",
      "             'demonstrated actions may be suboptimal in the real world, they '\n",
      "             \"may actually be near-optimal with respect to the user's internal \"\n",
      "             'model of the dynamics. By estimating these internal beliefs from '\n",
      "             'observed behavior, we arrive at a new method for inferring '\n",
      "             'intent. We demonstrate in simulation and in a user study with 12 '\n",
      "             'participants that this approach enables us to more accurately '\n",
      "             'model human intent, and can be used in a variety of '\n",
      "             'applications, including offering assistance in a shared autonomy '\n",
      "             'framework and inferring human preferences.',\n",
      " 'author': 'Sid Reddy, Anca Dragan, Sergey Levine',\n",
      " 'keywords': ['model human',\n",
      "              'inverse planning',\n",
      "              'inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2018/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf',\n",
      " 'paper': \"Where Do You Think You're Going?: Inferring Beliefs about Dynamics \"\n",
      "          'from Behavior',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2018/file/6f2268bd1d3d3ebaabb04d6b5d099425-Supplemental.zip',\n",
      " 'year': 2018}\n",
      "{'abstract': 'The design of a reward function often poses a major practical '\n",
      "             'challenge to real-world applications of reinforcement learning. '\n",
      "             'Approaches such as inverse reinforcement learning attempt to '\n",
      "             'overcome this challenge, but require expert demonstrations, '\n",
      "             'which can be difficult or expensive to obtain in practice. We '\n",
      "             'propose inverse event-based control, which generalizes inverse '\n",
      "             'reinforcement learning methods to cases where full '\n",
      "             'demonstrations are not needed, such as when only samples of '\n",
      "             'desired goal states are available. Our method is grounded in an '\n",
      "             'alternative perspective on control and reinforcement learning, '\n",
      "             \"where an agent's goal is to maximize the probability that one or \"\n",
      "             'more events will happen at some point in the future, rather than '\n",
      "             'maximizing cumulative rewards. We demonstrate the effectiveness '\n",
      "             'of our methods on continuous control tasks, with a focus on '\n",
      "             'high-dimensional observations like images where rewards are hard '\n",
      "             'or even impossible to specify.',\n",
      " 'author': 'Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, Sergey Levine',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2018/file/c9319967c038f9b923068dabdf60cfe3-Paper.pdf',\n",
      " 'paper': 'Variational Inverse Control with Events: A General Framework for '\n",
      "          'Data-Driven Reward Definition',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2018/file/c9319967c038f9b923068dabdf60cfe3-Supplemental.zip',\n",
      " 'year': 2018}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 'Inverse reinforcement learning (IRL) attempts to infer human '\n",
      "             'rewards or preferences from observed behavior. Since human '\n",
      "             'planning systematically deviates from rationality, several '\n",
      "             'approaches have been tried to account for specific human '\n",
      "             'shortcomings. \\n'\n",
      "             'However, the general problem of inferring the reward function of '\n",
      "             'an agent of unknown rationality has received little attention.\\n'\n",
      "             'Unlike the well-known ambiguity problems in IRL, this one is '\n",
      "             'practically relevant but cannot be resolved by observing the '\n",
      "             \"agent's policy in enough environments.\\n\"\n",
      "             'This paper shows (1) that a No Free Lunch result implies it is '\n",
      "             'impossible to uniquely decompose a policy into a planning '\n",
      "             'algorithm and reward function, and (2) that even with a '\n",
      "             \"reasonable simplicity prior/Occam's razor on the set of \"\n",
      "             'decompositions, we cannot distinguish between the true '\n",
      "             'decomposition and others that lead to high regret.\\n'\n",
      "             \"To address this, we need simple `normative' assumptions, which \"\n",
      "             'cannot be deduced exclusively from observations.',\n",
      " 'author': 'Stuart Armstrong, Sören Mindermann',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2018/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf',\n",
      " 'paper': \"Occam's razor is insufficient to infer the preferences of \"\n",
      "          'irrational agents',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2018/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Supplemental.zip',\n",
      " 'year': 2018}\n",
      "26\n",
      "{'abstract': 'We introduce a novel repeated Inverse Reinforcement Learning '\n",
      "             'problem: the agent has to act on behalf of a human in a sequence '\n",
      "             'of tasks and wishes to minimize the number of tasks that it '\n",
      "             'surprises the human by acting suboptimally with respect to how '\n",
      "             'the human would have acted. Each time the human is surprised, '\n",
      "             'the agent is provided a demonstration of the desired behavior by '\n",
      "             'the human. We formalize this problem, including how the sequence '\n",
      "             'of tasks is chosen, in a few different ways and provide some '\n",
      "             'foundational results.',\n",
      " 'author': 'Kareem Amin, Nan Jiang, Satinder Singh',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2017/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf',\n",
      " 'paper': 'Repeated Inverse Reinforcement Learning',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2017/file/8ce6790cc6a94e65f17f908f462fae85-Supplemental.zip',\n",
      " 'year': 2017}\n",
      "{'abstract': 'Inverse Reinforcement Learning (IRL) is an effective approach to '\n",
      "             'recover a reward function that explains the behavior of an '\n",
      "             'expert by observing a set of demonstrations.  This paper is '\n",
      "             'about a novel model-free IRL approach that, differently from '\n",
      "             'most of the existing IRL algorithms, does not require to specify '\n",
      "             \"a function space where to search for the expert's reward \"\n",
      "             'function. Leveraging on the fact that the policy gradient needs '\n",
      "             'to be zero for any optimal policy, the algorithm generates a set '\n",
      "             'of basis functions that span the subspace of reward functions '\n",
      "             'that make the policy gradient vanish. Within this subspace, '\n",
      "             'using a second-order criterion, we search for the reward '\n",
      "             \"function that penalizes the most a deviation from the expert's \"\n",
      "             'policy. After introducing our approach for finite domains, we '\n",
      "             'extend it to continuous ones. The proposed approach is '\n",
      "             'empirically compared to other IRL methods both in the (finite) '\n",
      "             'Taxi domain and in the (continuous) Linear Quadratic Gaussian '\n",
      "             '(LQG) and Car on the Hill environments.',\n",
      " 'author': 'Alberto Maria Metelli, Matteo Pirotta, Marcello Restelli',\n",
      " 'keywords': ['inverse reinforcement learning'],\n",
      " 'link': 'https://proceedings.neurips.cc/paper/2017/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf',\n",
      " 'paper': 'Compatible Reward Inverse Reinforcement Learning',\n",
      " 'supplemental': 'https://proceedings.neurips.cc/paper/2017/file/e6d8545daa42d5ced125a4bf747b3688-Supplemental.zip',\n",
      " 'year': 2017}\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "years = range(2017, 2021)\n",
    "keywords = ['model human', 'human navigation', 'theory of mind', \n",
    "            'inverse planning', 'inverse reinforcement learning',\n",
    "            'learn reward function', 'goal inference',\n",
    "            'infer the goal', 'infer the plan', 'plan recognition',\n",
    "            ]\n",
    "papers = []\n",
    "\n",
    "def get_abstract_link(URL):\n",
    "    link = 'None'\n",
    "    supplemental = 'None'\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    body = soup.find_all('div', class_='col')\n",
    "    if len(body) > 0:\n",
    "        body = body[0]\n",
    "        abstract = body.find_all('p')[-1].text\n",
    "        urls = body.find_all('a', class_='btn')\n",
    "        for url in urls:\n",
    "            url = url['href']\n",
    "            if 'Paper' in url:\n",
    "                link = 'https://proceedings.neurips.cc'+url\n",
    "            elif 'Supplemental' in url:\n",
    "                supplemental = 'https://proceedings.neurips.cc'+url\n",
    "        return abstract, link, supplemental\n",
    "    return \"\", \"\", \"\"\n",
    "\n",
    "for year in reversed(years):\n",
    "    URL = f'https://proceedings.neurips.cc/paper/{year}'\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    links = soup.find_all('div', class_='col')[0].find_all('li')\n",
    "    for link in links:\n",
    "        found = []\n",
    "        author = link.find_all('i')[0].text\n",
    "        a = link.find_all('a')[0]\n",
    "        paper = a.text\n",
    "        URL = 'https://proceedings.neurips.cc'+a['href']\n",
    "        abstract, link, supplemental = get_abstract_link(URL)\n",
    "        for keyword in keywords:\n",
    "            if keyword in abstract.lower():\n",
    "                found.append(keyword)\n",
    "        if len(found) > 0:\n",
    "            entry = {\n",
    "                'paper': paper,\n",
    "                'author': author,\n",
    "                'abstract': abstract,\n",
    "                'link': link,\n",
    "                'supplemental': supplemental,\n",
    "                'year': year,\n",
    "                'keywords': found\n",
    "            }\n",
    "            pprint(entry)\n",
    "            papers.append(entry)\n",
    "\n",
    "    ## for saving the summary as a json file, takes a long while\n",
    "    print(len(papers))\n",
    "    with open(f\"paper-{year}.json\", \"w\") as outfile:\n",
    "        json.dump(papers, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"papers.json\", \"w\") as outfile:\n",
    "    json.dump(papers, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'goal inference': 1,\n",
      " 'human navigation': 0,\n",
      " 'infer the goal': 1,\n",
      " 'infer the plan': 0,\n",
      " 'inverse planning': 1,\n",
      " 'inverse reinforcement learning': 24,\n",
      " 'learn reward function': 2,\n",
      " 'model human': 1,\n",
      " 'plan recognition': 0,\n",
      " 'theory of mind': 2}\n"
     ]
    }
   ],
   "source": [
    "## for printing a summary by keyword\n",
    "\n",
    "index = 0\n",
    "papers_by_keywords = {k:[] for k in keywords}\n",
    "for paper in papers:\n",
    "    for keyword in paper['keywords']:\n",
    "        papers_by_keywords[keyword].append(index)\n",
    "    index += 1\n",
    "pprint({k:len(v) for k,v in papers_by_keywords.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "## for downloading papers\n",
    "\n",
    "import urllib3\n",
    "import shutil\n",
    "\n",
    "c = urllib3.PoolManager()\n",
    "\n",
    "def download_paper(url, paper_name):\n",
    "    with c.request('GET',url, preload_content=False) as resp, open('papers/'+paper_name, 'wb') as out_file:\n",
    "        shutil.copyfileobj(resp, out_file)\n",
    "        resp.release_conn()\n",
    "    \n",
    "## for downloading the papers\n",
    "index = 0\n",
    "for paper in papers:\n",
    "    print(index, paper)\n",
    "    index += 1\n",
    "    download_paper(paper['link'], paper['paper']+'.pdf')\n",
    "    download_paper(paper['supplemental'], paper['paper']+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
